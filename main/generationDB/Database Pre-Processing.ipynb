{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTTextLine, LTChar\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-base')\n",
    "model = AutoModel.from_pretrained('intfloat/multilingual-e5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_element_texts_from_pdf(pdf_path):\n",
    "    header_footer_candidates = defaultdict(int)\n",
    "    page_element_texts = []\n",
    "    num_pages = 0\n",
    "\n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        num_pages += 1\n",
    "        page_texts = get_page_texts(page_layout, header_footer_candidates)\n",
    "        page_element_texts.append(page_texts)\n",
    "\n",
    "    header_footers = [k for k, v in header_footer_candidates.items() if v >= num_pages * 0.5]\n",
    "    cleaned_elements_text = clean_header_footers(page_element_texts, header_footers)\n",
    "\n",
    "    preprocessed_text = format_final_text(cleaned_elements_text)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "def get_page_texts(page_layout, header_footer_candidates):\n",
    "    page_texts = []\n",
    "    text_containers = [e for e in page_layout if isinstance(e, LTTextContainer)]\n",
    "\n",
    "    for index, element in enumerate(text_containers):\n",
    "        if is_skip_element(element):\n",
    "            continue\n",
    "        \n",
    "        element_text = format_element_text(element.get_text())\n",
    "        page_texts.append(element_text)\n",
    "        \n",
    "        if is_header_footer(index, len(text_containers)):\n",
    "            header_footer_candidates[element_text] += 1\n",
    "\n",
    "    return page_texts\n",
    "\n",
    "def is_skip_element(element):\n",
    "    first_line = next((line for i, line in enumerate(element) if i == 0 and isinstance(line, LTTextLine)), None)\n",
    "    if first_line:\n",
    "        first_char = next((char for j, char in enumerate(first_line) if j == 0 and isinstance(char, LTChar)), None)\n",
    "        if first_char and \"Italic\" in first_char.fontname and round(first_char.size) == 8:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_header_footer(index, total_elements):\n",
    "    return index <= 4 or index >= total_elements - 4\n",
    "\n",
    "def format_element_text(element_text):\n",
    "    return \" \".join(element_text.replace(\"\\n\", \" \").split())\n",
    "\n",
    "def clean_header_footers(page_element_texts, header_footers):\n",
    "    cleaned_texts = []\n",
    "    pattern = r'(\\d+ de \\d+)|Página \\d+ de \\d+'\n",
    "\n",
    "    for page in page_element_texts:\n",
    "        cleaned_page = [element_text for element_text in page if element_text not in header_footers and not re.search(pattern, element_text)]\n",
    "        cleaned_texts.extend(cleaned_page)\n",
    "    \n",
    "    return cleaned_texts\n",
    "\n",
    "def format_final_text(cleaned_elements_text):\n",
    "    punctuation = ['.', '!', '?', \":\", \";\", \"-\", \"\"]\n",
    "    updated_list = []\n",
    "    \n",
    "    for i, string in enumerate(cleaned_elements_text):\n",
    "        if i == len(cleaned_elements_text) - 1 or (cleaned_elements_text[i + 1] and not cleaned_elements_text[i + 1][0].islower()):\n",
    "            if string and string[-1] not in punctuation:\n",
    "                string += '.'\n",
    "        updated_list.append(string)\n",
    "\n",
    "    return updated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(string):\n",
    "    return (len(string.split()))\n",
    "\n",
    "def token_counter(string):\n",
    "    tokens = tokenizer(string, return_tensors='pt')\n",
    "    return tokens['input_ids'].shape[1]\n",
    "\n",
    "def split_string_at_word(input_string, word_limit):\n",
    "    words = input_string.split()\n",
    "    return ' '.join(words[:word_limit]), ' '.join(words[word_limit:])\n",
    "\n",
    "def normalize_strings(strings, lower=260, upper=320, prefer=280, max_tokens=512):\n",
    "    normalized = []\n",
    "    i = 0\n",
    "    while i < len(strings):\n",
    "        current_string = strings[i]\n",
    "        current_string_words = word_count(current_string)\n",
    "        current_string_tokens = token_counter(current_string)\n",
    "        string_bigger_than_max = current_string_tokens >= max_tokens\n",
    "\n",
    "        next_string = strings[i+1] if i+1 < len(strings) else None\n",
    "\n",
    "        article_condition = next_string and (next_string.lower().startswith(\"artículo\") or next_string.lower().startswith(\"articulo\") or next_string.startswith(\"Decreto\") or next_string.startswith(\"DECRETO\"))\n",
    "\n",
    "        if (lower < current_string_words < upper or article_condition) and not string_bigger_than_max:\n",
    "            normalized.append(current_string)\n",
    "            i += 1\n",
    "        elif next_string:\n",
    "            next_string_sent = sent_tokenize(next_string)\n",
    "            if (current_string_words + word_count(next_string_sent[0]) < upper) and not string_bigger_than_max:\n",
    "                strings[i] += \" \" + next_string_sent[0]\n",
    "                if len(next_string_sent) > 1:\n",
    "                    strings[i+1] = \" \".join(next_string_sent[1:])\n",
    "                else:\n",
    "                    del strings[i+1]\n",
    "            else:\n",
    "                if current_string_words > upper and not string_bigger_than_max:\n",
    "                    curr, strings[i] = split_string_at_word(current_string, prefer)\n",
    "                    normalized.append(curr)\n",
    "                elif string_bigger_than_max:\n",
    "                    coefficient = 0.9\n",
    "                    while string_bigger_than_max:\n",
    "                        temp1, temp2 = split_string_at_word(current_string, math.floor(coefficient * current_string_words * max_tokens / current_string_tokens))\n",
    "                        string_bigger_than_max = token_counter(temp1) >= max_tokens\n",
    "                        coefficient -= 0.1\n",
    "                    strings[i] = temp2\n",
    "                    normalized.append(temp1)\n",
    "                else:\n",
    "                    normalized.append(current_string)\n",
    "                    i += 1\n",
    "        else:\n",
    "            if string_bigger_than_max:\n",
    "                if current_string_words > upper and not string_bigger_than_max:\n",
    "                    curr, strings[i] = split_string_at_word(current_string, prefer)\n",
    "                    normalized.append(curr)\n",
    "                elif string_bigger_than_max:\n",
    "                    coefficient = 0.9\n",
    "                    while string_bigger_than_max:\n",
    "                        temp1, temp2 = split_string_at_word(current_string, math.floor(coefficient * current_string_words * max_tokens / current_string_tokens))\n",
    "                        string_bigger_than_max = token_counter(temp1) >= max_tokens\n",
    "                        coefficient -= 0.1\n",
    "                    strings[i] = temp2\n",
    "                    normalized.append(temp1)\n",
    "                else:\n",
    "                    normalized.append(current_string)\n",
    "                    break\n",
    "            else:\n",
    "                normalized.append(current_string)\n",
    "                break\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(pdf_path):\n",
    "    strings_from_doc = extract_element_texts_from_pdf(pdf_path)\n",
    "    chunks = normalize_strings(strings_from_doc)\n",
    "    for i in chunks:\n",
    "        print\n",
    "        if token_counter(i) >= 512:\n",
    "            raise Exception(\"Token Sequence Error\")\n",
    "    return chunks\n",
    "\n",
    "def process_pdfs_in_folder(root_folder):\n",
    "    for dir_path, _, filenames in os.walk(root_folder):\n",
    "        for file_name in filenames:\n",
    "            if file_name.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(dir_path, file_name)\n",
    "                json_path = os.path.join(dir_path, file_name.rsplit('.', 1)[0] + '.json')\n",
    "                if (os.path.exists(json_path)):\n",
    "                    print(\"File already exists\")\n",
    "                    continue\n",
    "                print(pdf_path)\n",
    "                chunks = preprocess_documents(pdf_path)\n",
    "                with open(json_path, 'w', encoding=\"utf-8\") as json_file:\n",
    "                    json.dump(chunks, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pdfs_in_folder(\"federal/leyes_federales\")\n",
    "process_pdfs_in_folder(\"federal/reglamentos_federales\")\n",
    "\n",
    "process_pdfs_in_folder(\"state/leyes_estatales\")\n",
    "process_pdfs_in_folder(\"state/reglamentos_estatales\")\n",
    "process_pdfs_in_folder(\"state/reglamentos_municipales/monterrey\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_json_files(folder):\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if (file.endswith('.json') or file.endswith('.index')) and not (\"schema\" in file):\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "                print(f'Removed file: {file_path}')\n",
    "\n",
    "delete_json_files(\"federal/leyes_federales\")\n",
    "delete_json_files(\"federal/reglamentos_federales\")\n",
    "\n",
    "delete_json_files(\"state/leyes_estatales\")\n",
    "delete_json_files(\"state/reglamentos_estatales\")\n",
    "delete_json_files(\"state/reglamentos_municipales/monterrey\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
